{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Example with Off-Policy Learners\n",
    "---\n",
    "This notebook provides an example of evaluation of several different off-policy learners with synthetic bandit feedback data.\n",
    "\n",
    "Our example with synthetic bandit data contains the follwoing four major steps:\n",
    "- (1) Synthetic Data Generation\n",
    "- (2) Off-Policy Learning\n",
    "- (3) Evaluation of Off-Policy Learners\n",
    "\n",
    "Please see [../examples/opl](../opl) for a more sophisticated example of the evaluation of off-policy learners with synthetic bandit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import (\n",
    "    SyntheticBanditDataset,\n",
    "    logistic_reward_function,\n",
    "    linear_reward_function,\n",
    "    linear_behavior_policy\n",
    ")\n",
    "from obp.policy import IPWLearner, NNPolicyLearner, Random\n",
    "from obp.ope import (\n",
    "    RegressionModel,\n",
    "    InverseProbabilityWeighting,\n",
    "    DirectMethod,\n",
    "    DoublyRobust\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Synthetic Data Generation\n",
    "We prepare easy-to-use synthetic data generator: `SyntheticBanditDataset` class in the dataset module.\n",
    "\n",
    "It takes number of actions (`n_actions`), dimension of context vectors (`dim_context`), reward function (`reward_function`), and behavior policy (`behavior_policy_function`) as inputs and generates a synthetic bandit dataset that can be used to evaluate the performance of decision making policies (obtained by `off-policy learning`) and OPE estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_rounds': 10000,\n",
       " 'n_actions': 10,\n",
       " 'context': array([[-0.20470766,  0.47894334, -0.51943872, -0.5557303 ,  1.96578057],\n",
       "        [ 1.39340583,  0.09290788,  0.28174615,  0.76902257,  1.24643474],\n",
       "        [ 1.00718936, -1.29622111,  0.27499163,  0.22891288,  1.35291684],\n",
       "        ...,\n",
       "        [-1.27028221,  0.80914602, -0.45084222,  0.47179511,  1.89401115],\n",
       "        [-0.68890924,  0.08857502, -0.56359347, -0.41135069,  0.65157486],\n",
       "        [ 0.51204121,  0.65384817, -1.98849253, -2.14429131, -0.34186901]]),\n",
       " 'action_context': array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),\n",
       " 'action': array([6, 3, 2, ..., 9, 3, 6]),\n",
       " 'position': None,\n",
       " 'reward': array([1, 1, 1, ..., 0, 1, 0]),\n",
       " 'expected_reward': array([[0.80210203, 0.73828559, 0.83199558, ..., 0.81190503, 0.70617705,\n",
       "         0.68985306],\n",
       "        [0.94119582, 0.93473317, 0.91345213, ..., 0.94140688, 0.93152449,\n",
       "         0.90132868],\n",
       "        [0.87248862, 0.67974991, 0.66965669, ..., 0.79229752, 0.82712978,\n",
       "         0.74923536],\n",
       "        ...,\n",
       "        [0.66717573, 0.81583571, 0.77012708, ..., 0.87757008, 0.57652468,\n",
       "         0.80629132],\n",
       "        [0.52526986, 0.39952563, 0.61892038, ..., 0.53610389, 0.49392728,\n",
       "         0.58408936],\n",
       "        [0.55375831, 0.11662199, 0.807396  , ..., 0.22532856, 0.42629292,\n",
       "         0.24120499]]),\n",
       " 'pscore': array([0.14065505, 0.11373945, 0.10335615, ..., 0.07250876, 0.11373945,\n",
       "        0.14065505])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a synthetic bandit dataset with 10 actions\n",
    "# we use `logistic function` as the reward function and `linear_behavior_policy` as the behavior policy.\n",
    "# one can define their own reward function and behavior policy such as nonlinear ones. \n",
    "dataset = SyntheticBanditDataset(\n",
    "    n_actions=10,\n",
    "    dim_context=5,\n",
    "    reward_type=\"binary\", # \"binary\" or \"continuous\"\n",
    "    reward_function=logistic_reward_function,\n",
    "    behavior_policy_function=linear_behavior_policy,\n",
    "    random_state=12345,\n",
    ")\n",
    "# obtain training and test sets of synthetic logged bandit feedback\n",
    "n_rounds_train, n_rounds_test = 10000, 10000\n",
    "bandit_feedback_train = dataset.obtain_batch_bandit_feedback(n_rounds=n_rounds_train)\n",
    "bandit_feedback_test = dataset.obtain_batch_bandit_feedback(n_rounds=n_rounds_test)\n",
    "\n",
    "# `bandit_feedback` is a dictionary storing synthetic logged bandit feedback\n",
    "bandit_feedback_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Off-Policy Learning\n",
    "After generating synthetic data, we now train some candidate evaluation policies using the training bandit dataset. <br>\n",
    "\n",
    "We use \"NN Policy Learner\" and *IPW Learner* implemented in the policy module to train evaluation policies. \n",
    "For NN Learner, we use *DirectMethod* (DM), *InverseProbabilityWeighting* (IPW), and *DoublyRobust* (DR) as objective functions.\n",
    "For IPW Learner, we use *RandomForestClassifier* and *LogisticRegression* implemented in scikit-learn for base machine learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the mean reward function by using ML model (Logistic Regression here)\n",
    "# the estimated rewards are used by model-dependent estimators such as DM and DR\n",
    "regression_model = RegressionModel(\n",
    "    n_actions=dataset.n_actions,\n",
    "    action_context=dataset.action_context,\n",
    "    base_model=LogisticRegression(random_state=12345),\n",
    ")\n",
    "# please refer to https://arxiv.org/abs/2002.08536 about the details of the cross-fitting procedure.\n",
    "estimated_rewards_by_reg_model = regression_model.fit_predict(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    n_folds=3, # use 3-fold cross-fitting\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuro/hanjuku/code/zr-obp/obp-venv/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    }
   ],
   "source": [
    "# define DM\n",
    "dm = DirectMethod()\n",
    "# define NNPolicyLearner with DM as its objective function\n",
    "nn_dm = NNPolicyLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    dim_context=5,\n",
    "    off_policy_objective=dm.estimate_policy_value_tensor,\n",
    "    random_state=12345,\n",
    ")\n",
    "# train NNPolicyLearner on the training set of the synthetic logged bandit feedback\n",
    "nn_dm.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n",
    ")\n",
    "# obtains action choice probabilities for the test set of the synthetic logged bandit feedback\n",
    "action_dist_nn_dm = nn_dm.predict_proba(context=bandit_feedback_test[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define IPW\n",
    "ipw = InverseProbabilityWeighting()\n",
    "# define NNPolicyLearner with IPW as its objective function\n",
    "nn_ipw = NNPolicyLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    dim_context=5,\n",
    "    off_policy_objective=ipw.estimate_policy_value_tensor,\n",
    "    random_state=12345,\n",
    ")\n",
    "# train NNPolicyLearner on the training set of the synthetic logged bandit feedback\n",
    "nn_ipw.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    pscore=bandit_feedback_train[\"pscore\"],\n",
    ")\n",
    "# obtains action choice probabilities for the test set of the synthetic logged bandit feedback\n",
    "action_dist_nn_ipw = nn_ipw.predict_proba(context=bandit_feedback_test[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DR\n",
    "dr = DoublyRobust()\n",
    "# define NNPolicyLearner with DR as its objective function\n",
    "nn_dr = NNPolicyLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    dim_context=5,\n",
    "    off_policy_objective=dr.estimate_policy_value_tensor,\n",
    "    random_state=12345,\n",
    ")\n",
    "# train NNPolicyLearner on the training set of the synthetic logged bandit feedback\n",
    "nn_dr.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    pscore=bandit_feedback_train[\"pscore\"],\n",
    "    estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n",
    ")\n",
    "# obtains action choice probabilities for the test set of the synthetic logged bandit feedback\n",
    "action_dist_nn_dr = nn_dr.predict_proba(context=bandit_feedback_test[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define IPWLearner with Logistic Regression as its base ML model\n",
    "ipw_lr = IPWLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    base_classifier=LogisticRegression(C=100, random_state=12345)\n",
    ")\n",
    "# train IPWLearner on the training set of the synthetic logged bandit feedback\n",
    "ipw_lr.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    pscore=bandit_feedback_train[\"pscore\"]\n",
    ")\n",
    "# obtains action choice probabilities for the test set of the synthetic logged bandit feedback\n",
    "action_dist_ipw_lr = ipw_lr.predict(context=bandit_feedback_test[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define IPWLearner with Random Forest as its base ML model\n",
    "ipw_rf = IPWLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    base_classifier=RandomForest(n_estimators=30, min_samples_leaf=10, random_state=12345)\n",
    ")\n",
    "# train IPWLearner on the training set of the synthetic logged bandit feedback\n",
    "ipw_rf.fit(\n",
    "    context=bandit_feedback_train[\"context\"],\n",
    "    action=bandit_feedback_train[\"action\"],\n",
    "    reward=bandit_feedback_train[\"reward\"],\n",
    "    pscore=bandit_feedback_train[\"pscore\"]\n",
    ")\n",
    "# obtains action choice probabilities for the test set of the synthetic logged bandit feedback\n",
    "action_dist_ipw_rf = ipw_rf.predict(context=bandit_feedback_test[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Uniform Random Policy as a baseline evaluation policy\n",
    "random = Random(n_actions=dataset.n_actions,)\n",
    "\n",
    "# compute the action choice probabililties for the test set of the synthetic logged bandit feedback\n",
    "action_dist_random = random.compute_batch_action_dist(\n",
    "    n_rounds=bandit_feedback_test[\"n_rounds\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Evaluation of Off-Policy Learners\n",
    "Our final step is the evaluation of off-policy learnres.\n",
    "With synthetic data, we can calculate the policy value of the off-policy learners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy value of NN Policy Learner with DM: 0.6785771195516228\n",
      "policy value of NN Policy Learner with IPW: 0.7429362678096227\n",
      "policy value of NN Policy Learner with DR: 0.7651217293062053\n",
      "policy value of IPW Learner with Logistic Regression: 0.767614655337475\n",
      "policy value of IPW Learner with Random Forest: 0.703809241480009\n",
      "policy value of Unifrom Random: 0.6043385526445931\n"
     ]
    }
   ],
   "source": [
    "# we first calculate the policy values of the three evaluation policies using the expected rewards of the test data\n",
    "policy_names = [\n",
    "    \"NN Policy Learner with DM\",\n",
    "    \"NN Policy Learner with IPW\",\n",
    "    \"NN Policy Learner with DR\",\n",
    "    \"IPW Learner with Logistic Regression\",\n",
    "    \"IPW Learner with Random Forest\",\n",
    "    \"Unifrom Random\"\n",
    "]\n",
    "action_dist_list = [\n",
    "    action_dist_nn_dm,\n",
    "    action_dist_nn_ipw,\n",
    "    action_dist_nn_dr,\n",
    "    action_dist_ipw_lr,\n",
    "    action_dist_ipw_rf,\n",
    "    action_dist_random\n",
    "]\n",
    "for name, action_dist in zip(policy_names, action_dist_list):\n",
    "    true_policy_value = dataset.calc_ground_truth_policy_value(\n",
    "        expected_reward=bandit_feedback_test[\"expected_reward\"],\n",
    "        action_dist=action_dist,\n",
    "    )\n",
    "    print(f'policy value of {name}: {true_policy_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, IPW Learner with Logistic Regression is the best, and NN Policy Learner with DR is the second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see [../examples/opl](../opl) for a more sophisticated example of the evaluation of off-policy learners with synthetic bandit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
